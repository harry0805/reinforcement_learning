{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb x11-utils\n",
    "!pip install pyvirtualdisplay==0.2.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install flappy-bird-gym"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import flappy_bird_gym\n",
    "from time import sleep\n",
    "from gym.utils.play import play\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = flappy_bird_gym.make('FlappyBird-v0')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=False, size=(1400, 900))\n",
    "_ = display.start()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video = VideoRecorder(env, 'results.mp4')\n",
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    action = np.random.choice([0,1], p=[0.9,0.1])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    video.capture_frame()\n",
    "env.close()\n",
    "video.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=False, size=(1400, 900))\n",
    "_ = display.start()"
   ],
   "metadata": {
    "id": "1ivHUWQinFK-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1666105244993,
     "user_tz": 240,
     "elapsed": 590,
     "user": {
      "displayName": "Watch Look",
      "userId": "13511047437237984037"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "error",
     "timestamp": 1666105266885,
     "user": {
      "displayName": "Watch Look",
      "userId": "13511047437237984037"
     },
     "user_tz": 240
    },
    "id": "pZlWtqFikywi",
    "outputId": "f87de40b-4d1d-4445-f80c-b21a8db5b340",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-507c674921f6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0.9\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0.1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mobs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minfo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrender\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0mvideo\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcapture_frame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/flappy_bird_gym/envs/flappy_bird_env_simple.py\u001B[0m in \u001B[0;36mrender\u001B[0;34m(self, mode)\u001B[0m\n\u001B[1;32m    161\u001B[0m                                                 \u001B[0mbird_color\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bird_color\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m                                                 \u001B[0mpipe_color\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pipe_color\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 163\u001B[0;31m                                                 background=self._bg_type)\n\u001B[0m\u001B[1;32m    164\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_renderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_game\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_renderer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_display\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/flappy_bird_gym/envs/renderer.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, screen_size, audio_on, bird_color, pipe_color, background)\u001B[0m\n\u001B[1;32m     75\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_audio_queue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0maudio_on\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 77\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msounds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_sounds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     78\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     79\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgame\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/flappy_bird_gym/envs/utils.py\u001B[0m in \u001B[0;36mload_sounds\u001B[0;34m()\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mload_sounds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mDict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpyg_mixer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSound\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m     \u001B[0;34m\"\"\" Loads and returns the audio assets of the game. \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 142\u001B[0;31m     \u001B[0mpyg_mixer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    143\u001B[0m     \u001B[0msounds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31merror\u001B[0m: ALSA: Couldn't open audio device: No such file or directory"
     ]
    }
   ],
   "source": [
    "video = VideoRecorder(env, 'results.mp4')\n",
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    action = np.random.choice([0,1], p=[0.9,0.1])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    video.capture_frame()\n",
    "env.close()\n",
    "video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fNro3pKkywj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0URpIL3Nkywj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0uwZZXOkywk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Flatten(input_shape=(1,states)))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Dpsvq2tkywk",
    "outputId": "f858b463-a58e-4423-d069-7f2a84780e45",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 722\n",
      "Trainable params: 722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(2,2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2oKpXxlkywk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYt1snvnkywl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRwakkp5kywl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dqn = build_agent(model, 2)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wz7i-z6dkywm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUhX5Oamkywm",
    "outputId": "cae46eda-4faa-4c69-e34b-853b7db2c70e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.069 - mae: 19.112 - mean_q: 38.119 - score: 0.000\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.209 - mae: 19.003 - mean_q: 37.832 - score: 0.000\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.172 - mae: 19.242 - mean_q: 38.312 - score: 0.000\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.155 - mae: 19.196 - mean_q: 38.226 - score: 0.000\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.161 - mae: 19.188 - mean_q: 38.208 - score: 0.000\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.159 - mae: 19.243 - mean_q: 38.318 - score: 0.000\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.147 - mae: 19.159 - mean_q: 38.154 - score: 0.000\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.141 - mae: 19.007 - mean_q: 37.851 - score: 0.000\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.141 - mae: 19.026 - mean_q: 37.890 - score: 0.000\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.143 - mae: 19.055 - mean_q: 37.947 - score: 0.000\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 36s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.141 - mae: 19.051 - mean_q: 37.939 - score: 0.000\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.137 - mae: 19.043 - mean_q: 37.922 - score: 0.000\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.140 - mae: 19.060 - mean_q: 37.955 - score: 0.000\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.150 - mae: 19.113 - mean_q: 38.060 - score: 0.000\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.150 - mae: 19.162 - mean_q: 38.161 - score: 0.000\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.156 - mae: 19.180 - mean_q: 38.191 - score: 0.000\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.155 - mae: 19.139 - mean_q: 38.113 - score: 0.000\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.154 - mae: 19.118 - mean_q: 38.070 - score: 0.000\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.157 - mae: 19.131 - mean_q: 38.096 - score: 0.000\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.166 - mae: 19.185 - mean_q: 38.203 - score: 0.000\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.163 - mae: 19.181 - mean_q: 38.195 - score: 0.000\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.171 - mae: 19.199 - mean_q: 38.231 - score: 0.000\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.160 - mae: 19.155 - mean_q: 38.145 - score: 0.000\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.161 - mae: 19.161 - mean_q: 38.155 - score: 0.000\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.162 - mae: 19.172 - mean_q: 38.175 - score: 0.000\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.157 - mae: 19.157 - mean_q: 38.149 - score: 0.000\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 46s 5ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.161 - mae: 19.161 - mean_q: 38.156 - score: 0.000\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.166 - mae: 19.177 - mean_q: 38.187 - score: 0.000\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.173 - mae: 19.336 - mean_q: 38.505 - score: 0.000\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.167 - mae: 19.255 - mean_q: 38.343 - score: 0.000\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.153 - mae: 19.187 - mean_q: 38.212 - score: 0.000\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.166 - mae: 19.241 - mean_q: 38.315 - score: 0.000\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.155 - mae: 19.198 - mean_q: 38.230 - score: 0.000\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.156 - mae: 19.205 - mean_q: 38.247 - score: 0.000\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.149 - mae: 19.123 - mean_q: 38.082 - score: 0.000\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.154 - mae: 19.196 - mean_q: 38.230 - score: 0.000\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.158 - mae: 19.230 - mean_q: 38.294 - score: 0.000\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.156 - mae: 19.198 - mean_q: 38.232 - score: 0.000\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.152 - mae: 19.120 - mean_q: 38.078 - score: 0.000\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.157 - mae: 19.168 - mean_q: 38.174 - score: 0.000\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.170 - mae: 19.246 - mean_q: 38.327 - score: 0.000\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 42s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.159 - mae: 19.180 - mean_q: 38.197 - score: 0.000\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 38s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.160 - mae: 19.153 - mean_q: 38.144 - score: 0.000\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.157 - mae: 19.146 - mean_q: 38.128 - score: 0.000\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 37s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.148 - mae: 19.137 - mean_q: 38.112 - score: 0.000\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.140 - mae: 19.041 - mean_q: 37.923 - score: 0.000\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 39s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.145 - mae: 19.116 - mean_q: 38.069 - score: 0.000\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 41s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.150 - mae: 19.109 - mean_q: 38.054 - score: 0.000\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 43s 4ms/step - reward: 1.0000\n",
      "99 episodes - episode_reward: 101.000 [101.000, 101.000] - loss: 0.125 - mae: 19.136 - mean_q: 38.124 - score: 0.000\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 40s 4ms/step - reward: 1.0000\n",
      "done, took 1962.739 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1a0ba0070>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=500000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1Y_keX6ikywn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# See the model with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oa7q3Gl-kywn",
    "outputId": "4a0ab1d5-863f-4dfa-9113-155df8dc6e17",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 101.000, steps: 101\n",
      "Episode 2: reward: 101.000, steps: 101\n",
      "Episode 3: reward: 101.000, steps: 101\n",
      "Episode 4: reward: 101.000, steps: 101\n",
      "Episode 5: reward: 101.000, steps: 101\n"
     ]
    }
   ],
   "source": [
    "t = dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aj7f9G6Okywn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "11b68844c66d1f2a6e6746a523554d5ba43f7ea52abca50f4e275a522b24def1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}